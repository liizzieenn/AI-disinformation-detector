import os
import time
import logging
import requests
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import re
from dotenv import load_dotenv
from groq import Groq

# Logging Setup 
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load API Keys from .env
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
NEWS_API_KEY = os.getenv("NEWS_API_KEY")
GNEWS_API_KEY = os.getenv("GNEWS_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")

REQUIRED_KEYS = {
    "GOOGLE_API_KEY": GOOGLE_API_KEY,
    "GOOGLE_CSE_ID": GOOGLE_CSE_ID,
    "GROQ_API_KEY": GROQ_API_KEY,
    "NEWS_API_KEY": NEWS_API_KEY,
    "GNEWS_API_KEY": GNEWS_API_KEY,
    "OPENAI_API_KEY": OPENAI_API_KEY,
    "TELEGRAM_BOT_TOKEN": TELEGRAM_BOT_TOKEN
}
missing = [k for k, v in REQUIRED_KEYS.items() if not v]
if missing:
    raise EnvironmentError(f"Missing API keys: {', '.join(missing)}")

# Initialize Groq
client = Groq(api_key=GROQ_API_KEY)


# Google Search 
def google_search(query, num_results=3, retries=3, backoff=1):
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": GOOGLE_API_KEY,
        "cx": GOOGLE_CSE_ID,
        "q": query,
        "num": num_results
    }
    for attempt in range(retries):
        try:
            res = requests.get(url, params=params, timeout=5)
            res.raise_for_status()
            items = res.json().get("items", [])
            return [{
                "title": item.get("title", "·É°·Éê·Éó·Éê·É£·É†·Éò ·Éê·É† ·Éê·É†·Éò·É°"),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", "")
            } for item in items]
        except requests.exceptions.RequestException as e:
            logging.warning(f"Google Search ·É®·Éî·É™·Éì·Éù·Éõ·Éê (·É™·Éì·Éê {attempt + 1}): {e}")
            if attempt < retries - 1:
                time.sleep(backoff * (2 ** attempt))
    return []


# NewsAPI Search 
def newsapi_search(query, language="ka", page_size=3):
    try:
        url = "https://newsapi.org/v2/everything"
        params = {
            "q": query,
            "language": language,
            "pageSize": page_size,
            "apiKey": NEWS_API_KEY
        }
        res = requests.get(url, params=params)
        res.raise_for_status()
        articles = res.json().get("articles", [])
        return [{
            "title": a["title"],
            "link": a["url"],
            "description": a.get("description", "")
        } for a in articles]
    except Exception as e:
        logging.warning(f"NewsAPI ·É®·Éî·É™·Éì·Éù·Éõ·Éê: {e}")
        return []


# GNews Search 
def gnews_search(query, max_results=3):
    try:
        url = f"https://gnews.io/api/v4/search?q={query}&lang=ka&token={GNEWS_API_KEY}&max={max_results}"
        res = requests.get(url)
        res.raise_for_status()
        articles = res.json().get("articles", [])
        return [{
            "title": a["title"],
            "link": a["url"],
            "description": a.get("description", "")
        } for a in articles]
    except Exception as e:
        logging.warning(f"GNews API ·É®·Éî·É™·Éì·Éù·Éõ·Éê: {e}")
        return []


# Parse Score 
def parse_score_and_label(ai_text):
    match = re.search(r"(\d{1,3})%", ai_text)
    score = int(match.group(1)) if match else 50
    if "·É†·Éî·Éê·Éö·É£·É†·Éò" in ai_text:
        label = "·É†·Éî·Éê·Éö·É£·É†·Éò ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê"
    elif "·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê" in ai_text:
        label = "·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê"
    else:
        label = "·Éí·Éê·É£·É†·Éô·Éï·Éî·Éï·Éî·Éö·Éò·Éê"
    return score, label


# Show Pie 
def show_score_pie(score):
    labels = ['·É†·Éî·Éê·Éö·É£·É†·Éò ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê', '·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê']
    sizes = [score, 100 - score]
    colors = ['#4CAF50', '#F44336']
    fig, ax = plt.subplots()
    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    ax.axis('equal')
    st.pyplot(fig)


# AI Analysis
def detect_disinformation_with_sources(text):
    google_results = google_search(text)
    news_results = newsapi_search(text)
    gnews_results = gnews_search(text)

    has_sources = any([google_results, news_results, gnews_results])

    if has_sources:
        combined_sources = (
            f"üîç Google ·É®·Éî·Éì·Éî·Éí·Éî·Éë·Éò:\n{google_results}\n\nüóû NewsAPI ·É°·Éò·Éê·ÉÆ·Éö·Éî·Éî·Éë·Éò:\n{news_results}\n\nüì∞ GNews ·É°·É¢·Éê·É¢·Éò·Éî·Éë·Éò:\n{gnews_results}"
        )
        prompt = f"""
·É®·Éî·Éú ·ÉÆ·Éê·É† ·Éú·Éî·Éò·É¢·É†·Éê·Éö·É£·É†·Éò, ·Éõ·Éô·Éê·É™·É†·Éò ·Éì·Éê ·Éê·Éô·Éê·Éì·Éî·Éõ·Éò·É£·É†·Éò ·É•·Éê·É†·Éó·É£·Éö·Éò ·Éî·Éú·Éò·É° ·ÉÆ·Éî·Éö·Éù·Éï·Éú·É£·É†·Éò ·Éò·Éú·É¢·Éî·Éö·Éî·É•·É¢·Éò.

·É®·Éî·Éê·ÉØ·Éê·Éõ·Éî ·É•·Éï·Éî·Éõ·Éù·Éó ·Éõ·Éù·Éß·Éï·Éê·Éú·Éò·Éö·Éò ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò ·Éì·Éê ·É®·Éî·Éê·É§·Éê·É°·Éî ·É¢·Éî·É•·É°·É¢·Éò ‚Äî ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê·Éê ·Éó·É£ ·Éê·É†·Éê.

·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò:
{combined_sources}

üìÑ ·É®·Éî·É°·Éê·É§·Éê·É°·Éî·Éë·Éî·Éö·Éò ·É¢·Éî·É•·É°·É¢·Éò:  
{text}

·É®·Éî·Éì·Éî·Éí·Éò ·É§·Éù·É†·Éõ·Éê·É¢·Éò·Éó:

üìä ·É®·Éî·É§·Éê·É°·Éî·Éë·Éê: ·É†·Éî·Éê·Éö·É£·É†·Éò ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê / ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê / ·Éí·Éê·É£·É†·Éô·Éï·Éî·Éï·Éî·Éö·Éò·Éê  
üî¢ ·Éì·Éù·Éú·Éî: 0-100%  
üßæ ·Éê·É†·Éí·É£·Éõ·Éî·Éú·É¢·Éò·É†·Éî·Éë·É£·Éö·Éò ·Éê·É¶·É¨·Éî·É†·Éê  
üîó ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò
"""
    else:
        prompt = f"""
·É®·Éî·Éú ·ÉÆ·Éê·É† ·Éú·Éî·Éò·É¢·É†·Éê·Éö·É£·É†·Éò, ·Éõ·Éô·Éê·É™·É†·Éò ·Éì·Éê ·Éê·Éô·Éê·Éì·Éî·Éõ·Éò·É£·É†·Éò ·É•·Éê·É†·Éó·É£·Éö·Éò ·Éî·Éú·Éò·É° ·ÉÆ·Éî·Éö·Éù·Éï·Éú·É£·É†·Éò ·Éò·Éú·É¢·Éî·Éö·Éî·É•·É¢·Éò.

·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò ·Éï·Éî·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê, ·É®·Éî·Éê·É§·Éê·É°·Éî ·Éõ·ÉÆ·Éù·Éö·Éù·Éì ·É¢·Éî·É•·É°·É¢·Éò·É° ·É°·Éê·É§·É£·É´·Éï·Éî·Éö·Éñ·Éî:

üìÑ ·É®·Éî·É°·Éê·É§·Éê·É°·Éî·Éë·Éî·Éö·Éò ·É¢·Éî·É•·É°·É¢·Éò:  
{text}

·É®·Éî·Éì·Éî·Éí·Éò ·É§·Éù·É†·Éõ·Éê·É¢·Éò·Éó:

üìä ·É®·Éî·É§·Éê·É°·Éî·Éë·Éê: ·É†·Éî·Éê·Éö·É£·É†·Éò ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê / ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê / ·Éí·Éê·É£·É†·Éô·Éï·Éî·Éï·Éî·Éö·Éò·Éê  
üî¢ ·Éì·Éù·Éú·Éî: 0-100%  
üßæ ·Éê·É†·Éí·É£·Éõ·Éî·Éú·É¢·Éò·É†·Éî·Éë·É£·Éö·Éò ·Éê·É¶·É¨·Éî·É†·Éê  
üîó ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò: "·É°·Éê·Éú·Éì·Éù ·É¶·Éò·Éê ·É¨·Éß·Éê·É†·Éù ·Éï·Éî·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê"
"""
    try:
        res = client.chat.completions.create(
            model="llama3-70b-8192",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=1500
        )
        ai_output = res.choices[0].message.content
        return ai_output, google_results, news_results, gnews_results
    except Exception as e:
        logging.error(f"Groq API ·É®·Éî·É™·Éì·Éù·Éõ·Éê: {e}")
        return "·É®·Éî·É™·Éì·Éù·Éõ·Éê ·Éõ·Éù·ÉÆ·Éì·Éê ·É®·Éî·É§·Éê·É°·Éî·Éë·Éò·É° ·Éû·É†·Éù·É™·Éî·É°·É®·Éò.", [], [], []


# Streamlit UI 
st.set_page_config(page_title="·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éò·É° ·Éì·Éî·É¢·Éî·É•·É¢·Éù·É†·Éò", page_icon="üß†")
st.title("üß† ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éò·É° ·Éì·Éî·É¢·Éî·É•·É¢·Éù·É†·Éò")
st.markdown("**·É©·Éê·É¨·Éî·É†·Éî ·É¢·Éî·É•·É°·É¢·Éò ·Éì·Éê ·É®·Éî·Éê·Éõ·Éù·É¨·Éî, ·Éê·É†·Éò·É° ·Éó·É£ ·Éê·É†·Éê ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê.**")

user_input = st.text_area("‚úçÔ∏è ·É¢·Éî·É•·É°·É¢·Éò ·É®·Éî·É°·Éê·Éõ·Éù·É¨·Éõ·Éî·Éë·Éö·Éê·Éì (·Éõ·Éê·É•·É°. 1000 ·É°·Éò·Éõ·Éë·Éù·Éö·Éù)", max_chars=1000, height=150)

if st.button("üîç ·É®·Éî·Éê·Éõ·Éù·É¨·Éî ·É¢·Éî·É•·É°·É¢·Éò"):
    if user_input.strip():
        with st.spinner("üîé ·Éî·É´·Éî·Éë·É° ·É¨·Éß·Éê·É†·Éù·Éî·Éë·É° ·Éì·Éê ·Éê·Éê·Éú·Éê·Éö·Éò·Éñ·Éî·Éë·É°..."):
            ai_result, google_results, news_results, gnews_results = detect_disinformation_with_sources(user_input)

        st.success("‚úÖ ·É®·Éî·É§·Éê·É°·Éî·Éë·Éê ·Éì·Éê·É°·É†·É£·Éö·Éì·Éê")

        st.markdown("### üìÑ AI ·É®·Éî·É§·Éê·É°·Éî·Éë·Éê:")
        st.markdown(ai_result)

        score, label = parse_score_and_label(ai_result)
        st.markdown(f"**·É®·Éî·É§·Éê·É°·Éî·Éë·Éê:** {label} ({score}%)")

        show_score_pie(score)
        st.markdown("---")


        def show_sources_table(results, title):
            if results:
                df = pd.DataFrame(results)
                df['title'] = df['title'].apply(lambda x: f"[{x}]")
                df = df.rename(
                    columns={'title': '·É°·Éê·Éó·Éê·É£·É†·Éò', 'link': '·Éö·Éò·Éú·Éô·Éò', 'snippet': '·Éê·É¶·É¨·Éî·É†·Éê', 'description': '·Éê·É¶·É¨·Éî·É†·Éê'})
                st.markdown(f"### üîó {title} ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò:")
                for _, row in df.iterrows():
                    st.markdown(f"**[{row['·É°·Éê·Éó·Éê·É£·É†·Éò'][1:-1]}]({row['·Éö·Éò·Éú·Éô·Éò']})**  \nüìÑ {row.get('·Éê·É¶·É¨·Éî·É†·Éê', '')}")
            else:
                st.markdown(f"### üîó {title} ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò: \n_·É®·Éî·Éì·Éî·Éí·Éò ·Éï·Éî·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê._")


        show_sources_table(google_results, "Google-·É®·Éò")
        show_sources_table(news_results, "NewsAPI-·Éñ·Éî")
        show_sources_table(gnews_results, "GNews-·Éñ·Éî")
    else:
        st.warning("‚ö†Ô∏è ·Éí·Éó·ÉÆ·Éù·Éï ·É®·Éî·Éò·Éß·Éï·Éê·Éú·Éî ·É¢·Éî·É•·É°·É¢·Éò ·É®·Éî·É°·Éê·Éõ·Éù·É¨·Éõ·Éî·Éë·Éö·Éê·Éì.")
