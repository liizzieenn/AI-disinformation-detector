import os
import time
import logging
import requests
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import re
from dotenv import load_dotenv
from groq import Groq

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Load .env ---
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
NEWS_API_KEY = os.getenv("NEWS_API_KEY")
GNEWS_API_KEY = os.getenv("GNEWS_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")

# --- Validate API Keys ---
REQUIRED_KEYS = {
    "GOOGLE_API_KEY": GOOGLE_API_KEY,
    "GOOGLE_CSE_ID": GOOGLE_CSE_ID,
    "GROQ_API_KEY": GROQ_API_KEY,
    "NEWS_API_KEY": NEWS_API_KEY,
    "GNEWS_API_KEY": GNEWS_API_KEY,
    "OPENAI_API_KEY": OPENAI_API_KEY,
    "TELEGRAM_BOT_TOKEN": TELEGRAM_BOT_TOKEN
}
missing = [k for k, v in REQUIRED_KEYS.items() if not v]
if missing:
    raise EnvironmentError(f"Missing API keys: {', '.join(missing)}")

# --- Initialize Groq client ---
client = Groq(api_key=GROQ_API_KEY)

# --- Google Custom Search ---
def google_search(query, num_results=3, retries=3, backoff=1):
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": GOOGLE_API_KEY,
        "cx": GOOGLE_CSE_ID,
        "q": query,
        "num": num_results
    }
    for attempt in range(retries):
        try:
            response = requests.get(url, params=params, timeout=5)
            response.raise_for_status()
            items = response.json().get("items", [])
            if not items:
                return []
            results = []
            for item in items:
                results.append({
                    "title": item.get("title", "·É°·Éê·Éó·Éê·É£·É†·Éò ·Éê·É† ·Éê·É†·Éò·É°"),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", "")
                })
            return results
        except requests.exceptions.RequestException as e:
            logging.warning(f"Google Search ·É®·Éî·É™·Éì·Éù·Éõ·Éê (·É™·Éì·Éê {attempt + 1}): {e}")
            if attempt < retries - 1:
                time.sleep(backoff * (2 ** attempt))
            else:
                return []

# --- NewsAPI Search ---
def newsapi_search(query, language="ka", page_size=3):
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": query,
        "language": language,
        "pageSize": page_size,
        "apiKey": NEWS_API_KEY
    }
    try:
        res = requests.get(url, params=params)
        res.raise_for_status()
        articles = res.json().get("articles", [])
        results = []
        for a in articles:
            results.append({
                "title": a["title"],
                "link": a["url"],
                "description": a.get("description", "")
            })
        return results
    except Exception as e:
        logging.warning(f"NewsAPI ·É®·Éî·É™·Éì·Éù·Éõ·Éê: {e}")
        return []

# --- GNews Search ---
def gnews_search(query, max_results=3):
    try:
        url = f"https://gnews.io/api/v4/search?q={query}&lang=ka&token={GNEWS_API_KEY}&max={max_results}"
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        articles = data.get("articles", [])
        results = []
        for a in articles:
            results.append({
                "title": a["title"],
                "link": a["url"],
                "description": a.get("description", "")
            })
        return results
    except Exception as e:
        logging.warning(f"GNews API ·É®·Éî·É™·Éì·Éù·Éõ·Éê: {e}")
        return []

# --- Parse AI Output for Score and Label ---
def parse_score_and_label(ai_text):
    # Try to extract percentage score from AI text
    match = re.search(r"(\d{1,3})%", ai_text)
    score = int(match.group(1)) if match else 50
    # Determine label heuristically
    if "·É†·Éî·Éê·Éö·É£·É†·Éò" in ai_text:
        label = "·É†·Éî·Éê·Éö·É£·É†·Éò ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê"
    elif "·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê" in ai_text:
        label = "·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê"
    else:
        label = "·Éí·Éê·É£·É†·Éô·Éï·Éî·Éï·Éî·Éö·Éò·Éê"
    return score, label

# --- Show Pie Chart for Score ---
def show_score_pie(score):
    labels = ['·É†·Éî·Éê·Éö·É£·É†·Éò ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê', '·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê']
    sizes = [score, 100 - score]
    colors = ['#4CAF50', '#F44336']
    fig, ax = plt.subplots()
    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    ax.axis('equal')
    st.pyplot(fig)

# --- Main AI Detection Function ---
def detect_disinformation_with_sources(text):
    google_results = google_search(text)
    news_results = newsapi_search(text)
    gnews_results = gnews_search(text)

    # ·É®·Éî·Éï·Éê·Éõ·Éù·É¨·Éõ·Éù·Éó ·Éí·Éï·Éê·É•·Éï·É° ·Éó·É£ ·Éê·É†·Éê ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò
    has_sources = any([
        google_results and "·É®·Éî·Éì·Éî·Éí·Éò ·Éê·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê" not in google_results,
        news_results and "·É®·Éî·Éì·Éî·Éí·Éò ·Éê·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê" not in news_results,
        gnews_results and "·É®·Éî·Éì·Éî·Éí·Éò ·Éê·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê" not in gnews_results
    ])

    if has_sources:
        combined_sources = (
            f"üîç Google ·É®·Éî·Éì·Éî·Éí·Éî·Éë·Éò:\n{google_results}\n\nüóû NewsAPI ·É°·Éò·Éê·ÉÆ·Éö·Éî·Éî·Éë·Éò:\n{news_results}\n\nüì∞ GNews ·É°·É¢·Éê·É¢·Éò·Éî·Éë·Éò:\n{gnews_results}"
        )
        prompt = f"""
·É®·Éî·Éú ·ÉÆ·Éê·É† ·Éú·Éî·Éò·É¢·É†·Éê·Éö·É£·É†·Éò, ·Éõ·Éô·Éê·É™·É†·Éò ·Éì·Éê ·Éê·Éô·Éê·Éì·Éî·Éõ·Éò·É£·É†·Éò ·É•·Éê·É†·Éó·É£·Éö·Éò ·Éî·Éú·Éò·É° ·ÉÆ·Éî·Éö·Éù·Éï·Éú·É£·É†·Éò ·Éò·Éú·É¢·Éî·Éö·Éî·É•·É¢·Éò.

·Éû·Éò·É†·Éï·Éî·Éö ·É†·Éò·Éí·É®·Éò ·É®·Éî·Éê·ÉØ·Éê·Éõ·Éî ·É•·Éï·Éî·Éõ·Éù·Éó ·Éõ·Éù·Éß·Éï·Éê·Éú·Éò·Éö·Éò ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò·Éì·Éê·Éú ·Éõ·Éò·É¶·Éî·Éë·É£·Éö·Éò ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê ·Éù·Éë·Éò·Éî·É•·É¢·É£·É†·Éê·Éì. 
·É®·Éî·Éõ·Éì·Éî·Éí ·É®·Éî·Éê·É§·Éê·É°·Éî ·É¢·Éî·É•·É°·É¢·Éò ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê·Éê ·Éó·É£ ·Éê·É†·Éê, ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éñ·Éî ·Éì·Éê·Éß·É†·Éì·Éú·Éù·Éë·Éò·Éó.

·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò:
{combined_sources}

üìÑ ·É®·Éî·É°·Éê·É§·Éê·É°·Éî·Éë·Éî·Éö·Éò ·É¢·Éî·É•·É°·É¢·Éò:  
{text}

·É®·Éî·Éì·Éî·Éí·Éò ·É§·Éù·É†·Éõ·Éê·É¢·Éò·Éó:

üìä ·É®·Éî·É§·Éê·É°·Éî·Éë·Éê: ·É†·Éî·Éê·Éö·É£·É†·Éò ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê / ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê / ·Éí·Éê·É£·É†·Éô·Éï·Éî·Éï·Éî·Éö·Éò·Éê  
üî¢ ·Éì·Éù·Éú·Éî: 0-100%  
üßæ ·Éê·É†·Éí·É£·Éõ·Éî·Éú·É¢·Éò·É†·Éî·Éë·É£·Éö·Éò ·Éê·É¶·É¨·Éî·É†·Éê: ·Éì·Éî·É¢·Éê·Éö·É£·É†·Éê·Éì ·Éê·ÉÆ·É°·Éî·Éú·Éò ·É®·Éî·Éú·Éò ·Éì·Éê·É°·Éô·Éï·Éú·Éê  
üîó ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò: ·Éõ·Éò·Éú·Éò·Éõ·É£·Éõ 2 ·É¨·Éß·Éê·É†·Éù ·Éê·Éú ·Éõ·Éò·É£·Éó·Éò·Éó·Éî ‚Äî "·É°·Éê·Éú·Éì·Éù ·É¶·Éò·Éê ·É¨·Éß·Éê·É†·Éù ·Éï·Éî·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê".
"""
    else:
        prompt = f"""
·É®·Éî·Éú ·ÉÆ·Éê·É† ·Éú·Éî·Éò·É¢·É†·Éê·Éö·É£·É†·Éò, ·Éõ·Éô·Éê·É™·É†·Éò ·Éì·Éê ·Éê·Éô·Éê·Éì·Éî·Éõ·Éò·É£·É†·Éò ·É•·Éê·É†·Éó·É£·Éö·Éò ·Éî·Éú·Éò·É° ·ÉÆ·Éî·Éö·Éù·Éï·Éú·É£·É†·Éò ·Éò·Éú·É¢·Éî·Éö·Éî·É•·É¢·Éò.

·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò ·Éê·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê, ·Éê·Éõ·Éò·É¢·Éù·Éõ ·Éõ·ÉÆ·Éù·Éö·Éù·Éì ·É¢·Éî·É•·É°·É¢·Éò·É° ·É°·Éê·É§·É£·É´·Éï·Éî·Éö·Éñ·Éî ·É®·Éî·Éê·É§·Éê·É°·Éî ·É¢·Éî·É•·É°·É¢·Éò ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê·Éê ·Éó·É£ ·Éê·É†·Éê.

üìÑ ·É®·Éî·É°·Éê·É§·Éê·É°·Éî·Éë·Éî·Éö·Éò ·É¢·Éî·É•·É°·É¢·Éò:  
{text}

·É®·Éî·Éì·Éî·Éí·Éò ·É§·Éù·É†·Éõ·Éê·É¢·Éò·Éó:

üìä ·É®·Éî·É§·Éê·É°·Éî·Éë·Éê: ·É†·Éî·Éê·Éö·É£·É†·Éò ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê / ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê / ·Éí·Éê·É£·É†·Éô·Éï·Éî·Éï·Éî·Éö·Éò·Éê  
üî¢ ·Éì·Éù·Éú·Éî: 0-100%  
üßæ ·Éê·É†·Éí·É£·Éõ·Éî·Éú·É¢·Éò·É†·Éî·Éë·É£·Éö·Éò ·Éê·É¶·É¨·Éî·É†·Éê: ·Éì·Éî·É¢·Éê·Éö·É£·É†·Éê·Éì ·Éê·ÉÆ·É°·Éî·Éú·Éò ·É®·Éî·Éú·Éò ·Éì·Éê·É°·Éô·Éï·Éú·Éê  
üîó ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò: "·É°·Éê·Éú·Éì·Éù ·É¶·Éò·Éê ·É¨·Éß·Éê·É†·Éù ·Éï·Éî·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê".
"""

    try:
        res = client.chat.completions.create(
            model="llama3-70b-8192",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=1500
        )
        return res.choices[0].message.content
    except Exception as e:
        logging.error(f"Groq API ·É®·Éî·É™·Éì·Éù·Éõ·Éê: {e}")
        return "·É®·Éî·É™·Éì·Éù·Éõ·Éê ·Éõ·Éù·ÉÆ·Éì·Éê ·É®·Éî·É§·Éê·É°·Éî·Éë·Éò·É° ·Éû·É†·Éù·É™·Éî·É°·É®·Éò."

# --- Streamlit UI ---
st.set_page_config(page_title="·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éò·É° ·Éì·Éî·É¢·Éî·É•·É¢·Éù·É†·Éò", page_icon="üß†")
st.title("üß† ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éò·É° ·Éì·Éî·É¢·Éî·É•·É¢·Éù·É†·Éò")
st.markdown("**·É©·Éê·É¨·Éî·É†·Éî ·É¢·Éî·É•·É°·É¢·Éò ·Éì·Éê ·É®·Éî·Éê·Éõ·Éù·É¨·Éî, ·Éê·É†·Éò·É° ·Éó·É£ ·Éê·É†·Éê ·Éì·Éî·Éñ·Éò·Éú·É§·Éù·É†·Éõ·Éê·É™·Éò·Éê.**")

user_input = st.text_area("‚úçÔ∏è ·É¢·Éî·É•·É°·É¢·Éò ·É®·Éî·É°·Éê·Éõ·Éù·É¨·Éõ·Éî·Éë·Éö·Éê·Éì (·Éõ·Éê·É•·É°. 1000 ·É°·Éò·Éõ·Éë·Éù·Éö·Éù)", max_chars=1000, height=150)

if st.button("üîç ·É®·Éî·Éê·Éõ·Éù·É¨·Éî ·É¢·Éî·É•·É°·É¢·Éò"):
    if user_input.strip():
        with st.spinner("üîé ·Éî·É´·Éî·Éë·É° ·É¨·Éß·Éê·É†·Éù·Éî·Éë·É° ·Éì·Éê ·Éê·Éê·Éú·Éê·Éö·Éò·Éñ·Éî·Éë·É°..."):
            ai_result, google_results, news_results, gnews_results = detect_disinformation_with_sources(user_input)

        st.success("‚úÖ ·É®·Éî·É§·Éê·É°·Éî·Éë·Éê ·Éì·Éê·É°·É†·É£·Éö·Éì·Éê")

        # Show AI result text
        st.markdown("### üìÑ AI ·É®·Éî·É§·Éê·É°·Éî·Éë·Éê:")
        st.markdown(ai_result)

        # Parse score & label for graphics
        score, label = parse_score_and_label(ai_result)
        st.markdown(f"**·É®·Éî·É§·Éê·É°·Éî·Éë·Éê:** {label} ({score}%)")

        # Show pie chart
        show_score_pie(score)

        st.markdown("---")

        # Show sources in tables
        def show_sources_table(results, title):
            if results:
                df = pd.DataFrame(results)
                df['title'] = df['title'].apply(lambda x: f"[{x}]")
                df = df.rename(columns={'title': '·É°·Éê·Éó·Éê·É£·É†·Éò', 'link': '·Éö·Éò·Éú·Éô·Éò', 'snippet': '·Éê·É¶·É¨·Éî·É†·Éê', 'description': '·Éê·É¶·É¨·Éî·É†·Éê'})
                # Make links clickable using st.markdown with unsafe_allow_html
                st.markdown(f"### üîó {title} ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò:")
                for idx, row in df.iterrows():
                    st.markdown(f"**[{row['·É°·Éê·Éó·Éê·É£·É†·Éò'][1:-1]}]({row['·Éö·Éò·Éú·Éô·Éò']})**  \nüìÑ {row.get('·Éê·É¶·É¨·Éî·É†·Éê', '')}")
            else:
                st.markdown(f"### üîó {title} ·É¨·Éß·Éê·É†·Éù·Éî·Éë·Éò: \n_·É®·Éî·Éì·Éî·Éí·Éò ·Éï·Éî·É† ·Éõ·Éù·Éò·É´·Éî·Éë·Éú·Éê._")

        show_sources_table(google_results, "Google-·É®·Éò")
        show_sources_table(news_results, "NewsAPI-·Éñ·Éî")
        show_sources_table(gnews_results, "GNews-·Éñ·Éî")

    else:
        st.warning("‚ö†Ô∏è ·Éí·Éó·ÉÆ·Éù·Éï ·É®·Éî·Éò·Éß·Éï·Éê·Éú·Éî ·É¢·Éî·É•·É°·É¢·Éò ·É®·Éî·É°·Éê·Éõ·Éù·É¨·Éõ·Éî·Éë·Éö·Éê·Éì.")
